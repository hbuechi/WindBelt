---
title: "WindBelt Regressions"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "WindBelt Regressions"
author: "WindBelt GP"
date: "January 13, 2018"
output: Word Document
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Outline of lexisUni text analysis (checked if done):
 1. Dowload pdf from lexisUni - search terms: "Master Project Name" AND "Project Developer" AND "wind" AND "energy' (if necessary, the state was added in the search term if location was not precise enough. 
 2. Save pdf in sample_pdfs folder x
      Each pdf saved as "fulltext"+ # search results if <10" + "abbreved project developer" "# of results pages in         >10", collapsed with "_" x
2. Create a df with everypdf in each row. my_data x
3. Create df with unested text unested,such that each row is a pdf page.x
4. Created new df that splits each page by word, sch that every row is a word in the text (unest_tokens where tokens are pdf) x
5. Get word count of specific words through group_by()
Conduct sentiment analysis on unique words x 
6. get hits and number of hits of different 'negative words' x
# Following meeting 10/22/18:
1. Unest token by group of words or sentences and conduct sentiment analysis on this.
2. Clean scripts - ID words in pdf that consistently pop up and need to be filtered out.
3. separate headlines from text to ensure we don't have duplicates
4. create csv format (NAME, Developer, State, Sentiment, subjectivity ...)
Other notes: 
*tidytext::tokenize function - every element in list become df. rbind dfs 
 str_count() how many times does a search term come up 
 str_match()
 regex() 
 
```{r load_packages}
### Packages 
library(pdftools)
library(tm)
library(devtools)
library(tidytext)
library(broom)
library(data.table)
library(tidyverse)
library(purrr)
library(googledrive)
library(knitr)
library(readr)
library(stringr)
# install.packages("tidyverse")
# install.packages("tidytext")
# library(tm.plugin.lexisnexis)
```


```{r connect_to_googledrive}
# The google drive folder id is simply the id in the folder url after the last slash
# So, in this example, the id here is derived from https://drive.google.com/drive/folders/1kZuJF3eS7SIiC8VBeGc6vVNvpBZHLnxg?ogsrc=32

# Create folder in desktop for pdfs. Decided to set on desktop to be compatible with all computers.
# If working on a Bren computer, use this:
NU_PDFS_R <- "H:/Desktop/NU_PDFS_R"

# Alex's Directory:
NU_PDFS_R <- "C:/Users/airvi/Documents/Bren/GroupProject/NU_PDFS_R"

dir.create(NU_PDFS_R, showWarnings = TRUE) # if directory already exists, will give warning. Ignore.

# Pull all pdfs directly from Google Drive 
NU_PDFs_R_id  <- "1alXSN-uUouUNM2cTHq5OS3LSxVhSDa_v"
NU_PDFs_R_folder <- googledrive::drive_ls(googledrive::as_id(NU_PDFs_R_id))
View(NU_PDFs_R_folder)


# function to download all lexisUni pdfs
pdf_downloader <- function(templates_dribble, local_folder){
  # download all pdfs
  
  for (i in 1:nrow(templates_dribble)){
    drive_download(as_id(templates_dribble$id[[i]]), 
                   file.path(local_folder, templates_dribble$name[[i]]),
                   overwrite = FALSE) #check if overwrite is needed here
  }
}

pdf_downloader(NU_PDFs_R_folder, NU_PDFS_R)
#function takes a while, since its pulling all 349 pdfs from googledrive
```



```{r import_data}
##### Ventyx ####
#Before running this code, make sure you have the Ventyx dataset saved on your desktop. The file path that the "read_csv" function accesses should be where the file is located on your desktop.

#For MAC and BREN computers:
ventyx_projects <- read_csv("~/Desktop/ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv")

#For Alex:
ventyx_projects <- read_csv("C:/Users/airvi/Desktop/ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv")

#To view the dataset:
View(ventyx_projects)

#To change the 0-1 definition to Low-High
ventyx_df <- mutate(WindBelt_Full, H_L_W = ifelse(lr_tif>0, "Low", "High"))


#### Google News ######
#Before running this code, make sure you have the Google scraping dataset saved on your desktop. The file path that the "read_csv" function accesses should be where the file is located on your desktop. 

#For MAC and BREN computers:
google_df <- read_csv("~/Desktop/google_scraping_01-08-19.csv")

#For Alex:
google_df <- read_csv("C:/Users/airvi/Desktop/google_scraping_01-08-19.csv")

#To view the dataset:
View(google_df)
```



```{r nexus_directory_setup}
#For MAC and BREN computers:
pdf_directory <- '~/Desktop/NU_PDFS_R'

#For Alex:
pdf_directory <- "C:/Users/airvi/Documents/Bren/GroupProject/NU_PDFS_R"

#For Delaney:
pdf_directory <- "D:/Desktop/All_LexisUni_PDFs"
pdf_directory <- "~/Desktop/All_LexisUni_PDFs"

#Listing all PDFs: should be 349 PDFs
pdfs <- paste(pdf_directory, "/", list.files(pdf_directory, pattern = "*.pdf", ignore.case = T), sep = "")

#PDF names
pdfs_names <- list.files(pdf_directory, pattern = "*.pdf", ignore.case = T)

#PDF text
pdfs_text <- purrr::map(pdfs, pdftools::pdf_text)

#Takes a minute...
#Expect 9 'PDF error' and ignore. Text normally still processed
```

#### Create initial dataframes that include document name and text
```{r dataframe_creation}
##### NU ####
#This combines the pdfs_names and pdfs_texts variables from the previous code chunk, into a single dataframe
#Each row is a PDF doc name with the full pdf text. Note: in the text column, each row is an element of a list
projects_NU <- data_frame(document = pdfs_names, text = pdfs_text)

#### Google ####
#This creates a data frame of just the project name and full text for each Google News article
#Each row is the project name with the full article text
projects_google <- data_frame(document = google_df$ProjectName, text = google_df$FullText)
```

#### Split text by page (only NU)
```{r page_split}
#Dataset with each page in one row
project_pdfpages_NU <- projects_NU %>% 
  unnest() # splits pdf text by page and removes list format ( c("")) since each element is now its own row.
View(project_pdfpages_NU)
```

#### Split text by word (unnest_tokens())
```{r word_split}
#### NU ####
#Dataset with each word in a row associated with its pdf source
#Also filters out unwanted words
projects_words_NU <- projects_NU %>% 
  unnest() %>% 
  tidytext::unnest_tokens(output = word, input = text, token = 
                          "words", to_lower = T) %>%      
  filter(!word %in% c("lexis",
                      "nexis", 
                      "Uni",
                      "about lexisnexis",
                      "Privacy Policy",
                      "Terms & Conditions", 
                      "Copyright Â© 2018 LexisNexis",
                      " | ",  
                      "@", 
                      "lexisnexis")) 

# %>% gsub("[^A-Za-z0-9,;._-]","")

#projects_pdfnest_NU <- projects_pdftext %>% 
#  unnest() %>% 
#  tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T)

# note: unnest_tokens() splits text by respective element (ie word, phrase, ...) word is default


#### Google #####
#Dataset with each word in a row associated with its project source 
projects_words_google <- projects_google %>% 
  tidytext::unnest_tokens(output = word, input = text, token = "words", to_lower = F)
```


#### Group words by pdf/project and summarize by frequency
```{r group_by}
#### NU ####
projects_words_count_NU <- projects_words_NU %>%
  group_by(document, word) %>% 
  summarise(count = n())
#View(projects_pdfwords_count)
#Counts the number of time a specific words is found in the pdf page

# projects_pdfnest_count <- projects_pdfnest_NU %>%
#   group_by(document, ngrams) %>% 
#   summarise(count = n())
#View(projects_pdfnest_count)
#add new count column with most freq. words


#### Google ####
projects_words_count_google <- projects_words_google %>%
  group_by(document, word) %>% 
  summarise(count = n())
#View(projects_pdfwords_count)
#Counts the number of time a specific words is found in the article
```

#### Sentiment dictionaries
```{r sentiment_dictionaries}
# Using 'afinn' vs. 'nrc sentiment tests.
get_sentiments("afinn") # associates word with a sentiment score
#afinn scores/ranks from -5 to +5 for positive or negative sentiment.

get_sentiments("nrc") # associated word with another sentiment feeling word

# View(get_sentiments("afinn"))
# View(get_sentiments("nrc"))

# We want scores not categorized words, so we will use AFINN for now.
```

#### Bind Sentiments
```{r bind_sentiment}
#### NU ####
projects_score_bind_NU <-projects_words_count_NU %>% 
  left_join(get_sentiments("afinn"), by = "word")

View(projects_score_bind_NU)

# Note: Many of the scores per words are NA simply because that word does not exist. 


#### Google ####
projects_score_bind <- projects_words_count_google %>% 
  left_join(get_sentiments("afinn"), by = "word")

View(projects_score_bind)

# Note: Many of the scores per words are NA simply because that word does not exist. 
```

#### Determine Project Scores
```{r projectscores}
#To determine the total score for each document (NU) or project (Google)

##### NU ######
total_sentiment_with_stats_NU <- projects_score_bind_NU %>% 
  filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score))

total_sentiment_NU <- projects_score_bind_NU %>% 
  filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = mean(score))
  
View(total_sentiment_with_stats_NU)
View(total_sentiment_NU)

#### Google #####
total_sentiment_with_stats_google <- projects_score_bind_google %>% 
  filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score)
            )

total_sentiment_google <- projects_score_bind_google %>% 
  filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = mean(score))
  
View(total_sentiment_with_stats_google)
View(total_sentiment_google)

#### NU ####
# write.csv(total_sentiment_NU, file = "Windbelt_sen.csv")

dir.create("G:/Data/VENTYX_DATASET/R_output_folders")
write.csv(total_sentiment_NU, file = "G:/Data/VENTYX_DATASET/R_output_folders/Windbelt_sen_NU.csv")

# Windbelt_sen_NU <- read_csv("D:/Desktop/Windbelt_sen.csv")
Windbelt_sen <- read.csv("G:/Data/VENTYX_DATASET/R_output_folders/Windbelt_sen_NU.csv")


#### FOR GOOGLE #### 
write.csv(total_sentiment_google, file = "Windbelt_sen_google.csv")

Windbelt_sen_google <- read_csv("G:/TextAnalysis/googlenews/Windbelt_sen_google.csv")

```

NOTE! I stopped doubling script for Google versus NU here.tbc. 

```{r combinewithsen}

#### NU ####
total_sentiment_df = as.data.frame(total_sentiment_NU)

windbelt_Full_w_sen <- merge(wind_df, total_sentiment_df, by.x ="ProjectName", by.y = "document")

windbelt_Full_w_sen <- rename(windbelt_Full_w_sen, sentiment=totals)

View(total_sentiment)
View(wind_df)
View(windbelt_Full_w_sen)


#### Google #####
total_sentiment_df = as.data.frame(total_sentiment_google)

windbelt_Full_w_sen <- merge(wind_df, total_sentiment_df, by.x ="ProjectName", by.y = "document")

windbelt_Full_w_sen <- rename(windbelt_Full_w_sen, sentiment=totals)

View(total_sentiment)
View(wind_df)
View(windbelt_Full_w_sen)



```


```{r pos_or_neg_sen}

Windbelt_P_or_N <- mutate(windbelt_Full_w_sen, p_n = ifelse(sentiment>0, "pos", "neg"))

View(Windbelt_P_or_N)

```



```{r organize_for_regressions}

#for pure sen scores

#na.omit(windbelt_Full_w_sen$Household_MeanIncome)
#na.omit(windbelt_Full_w_sen$PopDensity_mi)
#na.omit(windbelt_Full_w_sen$lr_tif)
#na.omit(windbelt_Full_w_sen$View_Score)

windbelt_Full_w_sen$H_L_W <- as.factor(windbelt_Full_w_sen$H_L_W)


windbelt_Full_w_sen$PopDensity_mi <- as.numeric(windbelt_Full_w_sen$PopDensity_mi)

windbelt_Full_w_sen$Household_MeanIncome <- as.numeric(windbelt_Full_w_sen$Household_MedianIncome)
windbelt_Full_w_sen$View_Score <- as.numeric(windbelt_Full_w_sen$View_Score)


windbelt_Full_w_sen$L_H_W <- relevel(windbelt_Full_w_sen$H_L_W, ref = "High")

str(windbelt_Full_w_sen)

summary(windbelt_Full_w_sen)


#for Neg or Pos sen scores

Windbelt_P_or_N$p_n <- as.factor(Windbelt_P_or_N$p_n)

Windbelt_P_or_N$H_L_W <- as.factor(Windbelt_P_or_N$H_L_W)


Windbelt_P_or_N$PopDensity_mi <- as.numeric(Windbelt_P_or_N$PopDensity_mi)

Windbelt_P_or_N$lr_tif <- as.factor(Windbelt_P_or_N$lr_tif)

Windbelt_P_or_N$Household_MeanIncome <- as.numeric(Windbelt_P_or_N$Household_MedianIncome)
Windbelt_P_or_N$View_Score <- as.numeric(Windbelt_P_or_N$View_Score)


Windbelt_P_or_N$L_H_W <- relevel(Windbelt_P_or_N$H_L_W, ref = "Low")

str(Windbelt_P_or_N)

summary(Windbelt_P_or_N)


```

```{r regression}

#With Pure Sen Scores

reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*sentiment, data=windbelt_Full_w_sen)
summary(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter)

reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_Inter <- lm(TimelineDays ~ View_Score + sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + H_L_W*sentiment, data=windbelt_Full_w_sen)
summary(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_Inter)

#reg_TL__HL_VS_SEN <- lm(TimelineDays ~ H_L_W + View_Score + sentiment + Capacity, data=windbelt_Full_w_sen)
#summary(reg_TL__HL_VS_SEN)

reg_TL_SEN <- lm(TimelineDays ~ sentiment, data=windbelt_Full_w_sen)
summary(reg_TL_SEN)

reg_TL_HL <- lm(TimelineDays ~ H_L_W, data=windbelt_Full_w_sen)
summary(reg_TL_HL)

reg_TL_VS <- lm(TimelineDays ~ View_Score, data=windbelt_Full_w_sen)
summary(reg_TL_VS)

reg_TL_POPDEN <- lm(TimelineDays ~ PopDensity_mi, data=windbelt_Full_w_sen)
summary(reg_TL_POPDEN)

reg_TL_MedianIncome <- lm(TimelineDays ~ Household_MedianIncome, data=windbelt_Full_w_sen)
summary(reg_TL_MedianIncome)

reg_SEN_VS <- lm(sentiment ~ View_Score, data=windbelt_Full_w_sen)
summary(reg_SEN_VS)

reg_SEN_HL <- lm(sentiment ~ H_L_W, data=windbelt_Full_w_sen)
summary(reg_SEN_HL)

#reg_TL__State_HL_VS_Sen_POPDEN <- lm(TimelineDays ~ State + H_L_W + View_Score + sentiment + PopDensity_mi, data=windbelt_Full_w_sen)
#summary(reg_TL__State_HL_VS_Sen_POPDEN)

#With positive/0 or negative sen
reg_PN_TL_HL_VS_POPDEN_Cap_MedianInc_State_INter <- lm(TimelineDays ~ H_L_W + View_Score + p_n + PopDensity_mi + Household_MedianIncome + Capacity + State + H_L_W*p_n, data=Windbelt_P_or_N)
summary(reg_PN_TL_HL_VS_POPDEN_Cap_MedianInc_State_INter)

reg_PN_TL__HL_VS_SEN <- lm(TimelineDays ~ H_L_W + View_Score + Pos_or_Neg, data=Windbelt_P_or_N)
summary(reg_PN_TL_HL_VS_SEN)

reg_PN_TL__SEN <- lm(TimelineDays ~ Pos_or_Neg, data=Windbelt_P_or_N)
summary(reg_PN_TL_SEN)

reg_PN_TL__HL <- lm(TimelineDays ~ H_L_W, data=Windbelt_P_or_N)
summary(reg_PN_TL_HL)

reg_PN_TL__VS <- lm(TimelineDays ~ View_Score, data=Windbelt_P_or_N)
summary(reg_PN_TL_VS)

reg_PN_TL__POPDEN <- lm(TimelineDays ~ PopDensity_mi, data=Windbelt_P_or_N)
summary(reg_PN_TL_POPDEN)

reg_PN_SEN__VS <- lm(sentiment ~ View_Score, data=Windbelt_P_or_N)
summary(reg_PN_SEN_VS)

reg_PN_SEN__HL <- lm(sentiment ~ H_L_W, data=Windbelt_P_or_N)
summary(reg_PN_SEN_HL)

reg_PN_TL__State_HL_VS_Sen_POPDEN <- lm(TimelineDays ~ State + H_L_W + View_Score + Pos_or_Neg + PopDensity_mi, data=Windbelt_P_or_N)
summary(reg_PN_TL_State_HL_VS_Sen_POPDEN)


```
```{r ttest_H_L}



```


#### Populate with negative words 
```{r negative_words}


negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
  p <- purrr::as_mapper(~identical(., character(0)))
  x[p(x)] <- NA
  return(x)
}
projects_pdftext_3 <- projects_pdftext_NU %>%
  mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
           map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
           map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
           tolower) # all our keywords are lower case
projects_pdftext_grouped <- projects_pdftext_2 %>%
  group_by(document, query_hits)
# OR 
my_data1grouped <- my_data1 %>%
  group_by(document, query_hits)%>%
  summarise(word_list = glue::glue_collapse(query_hits, sep = ";"))
View(projects_pdftext_2)
```